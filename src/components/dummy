// import { useState } from 'react';
// import { generateScript, generateImage, generateAudio } from '../services/openaiService';
// import { useNavigate } from 'react-router-dom';

// const VideoRequestForm = () => {
//   const navigate = useNavigate();
//   const [title, setTitle] = useState('');
//   const [isLoading, setIsLoading] = useState(false);
//   const [error, setError] = useState('');
//   const [scenes, setScenes] = useState([]);
//   const [generatedImages, setGeneratedImages] = useState([]);
//   const [generatedAudio, setGeneratedAudio] = useState([]);
//   const [videoProgress, setVideoProgress] = useState(0);

//   const handleSubmit = async (e) => {
//     e.preventDefault();
//     setIsLoading(true);
//     setError('');
    
//     try {
//       // Generate script
//       const scriptResponse = await generateScript(title);
//       const scriptData = JSON.parse(scriptResponse);
//       setScenes(scriptData.scenes);

//       // Generate all assets in parallel
//       const [images, audioUrls] = await Promise.all([
//         Promise.all(scriptData.scenes.map(scene => generateImage(scene.imagePrompt))),
//         Promise.all(scriptData.scenes.map(scene => generateAudio(scene.contentText)))
//       ]);

//       setGeneratedImages(images);
//       setGeneratedAudio(audioUrls);

//       // Navigate to video page with the generated content
//       navigate('/video', {
//         state: {
//           scenes: scriptData.scenes,
//           images,
//           audioUrls
//         }
//       });

//     } catch (error) {
//       console.error('Error:', error);
//       setError('Failed to generate video content. Please try again.');
//     } finally {
//       setIsLoading(false);
//     }
//   };

//   return (
//     <div className="max-w-4xl mx-auto p-6">
//       <form onSubmit={handleSubmit} className="mb-8">
//         <div className="mb-4">
//           <label 
//             htmlFor="title" 
//             className="block text-lg font-medium mb-2"
//           >
//             Enter Video Topic
//           </label>
//           <input
//             id="title"
//             type="text"
//             value={title}
//             onChange={(e) => setTitle(e.target.value)}
//             placeholder="e.g., The History of Ancient Rome"
//             className="w-full p-3 border rounded-lg"
//             disabled={isLoading}
//           />
//         </div>
//         <button
//           type="submit"
//           disabled={isLoading || !title.trim()}
//           className={`w-full p-3 rounded-lg text-white font-medium
//             ${isLoading || !title.trim() 
//               ? 'bg-gray-400' 
//               : 'bg-blue-600 hover:bg-blue-700'}`}
//         >
//           {isLoading ? 'Generating...' : 'Generate Video Content'}
//         </button>
//       </form>

//       {error && (
//         <div className="p-4 mb-4 text-red-700 bg-red-100 rounded-lg">
//           {error}
//         </div>
//       )}

//       {scenes.length > 0 && (
//         <div className="space-y-8">
//           {scenes.map((scene, index) => (
//             <div key={index} className="border rounded-lg p-6">
//               <h3 className="text-xl font-bold mb-4">Scene {index + 1}</h3>
//               <div className="mb-4">
//                 <h4 className="font-medium mb-2">Narration:</h4>
//                 <p className="text-gray-700">{scene.contentText}</p>
//               </div>
//               <div className="mb-4">
//                 <h4 className="font-medium mb-2">Image Description:</h4>
//                 <p className="text-gray-700">{scene.imagePrompt}</p>
//               </div>
//               {generatedImages[index] && (
//                 <div>
//                   <h4 className="font-medium mb-2">Generated Image:</h4>
//                   <img 
//                     src={generatedImages[index]} 
//                     alt={`Scene ${index + 1}`}
//                     className="w-full rounded-lg"
//                   />
//                 </div>
//               )}
//               {generatedAudio[index] && (
//                 <div className="mt-4">
//                   <h4 className="font-medium mb-2">Generated Audio:</h4>
//                   <audio 
//                     controls 
//                     src={generatedAudio[index]}
//                     className="w-full"
//                   >
//                     Your browser does not support the audio element.
//                   </audio>
//                 </div>
//               )}
//             </div>
//           ))}
//         </div>
//       )}

//       {isLoading && videoProgress > 0 && (
//         <div className="mt-4">
//           <div className="w-full bg-gray-200 rounded-full h-2.5">
//             <div 
//               className="bg-blue-600 h-2.5 rounded-full" 
//               style={{ width: `${videoProgress}%` }}
//             ></div>
//           </div>
//           <p className="text-center mt-2">Creating video: {videoProgress}%</p>
//         </div>
//       )}
//     </div>
//   );
// };

// export default VideoRequestForm; 

//dsadad

// export const generateSpeech = async (contentText) => {
//   try {
//     const response = await fetch('https://api.openai.com/v1/audio/speech', {
//       method: 'POST',
//       headers: {
//         'Content-Type': 'application/json',
//         'Authorization': process.env.AUDIO_API_KEY
//       },
//       body: JSON.stringify({
//         model: 'tts-1',
//         voice: 'alloy',
//         input: contentText,
//         speed: 1.15, // Consistent speed
//         response_format: 'mp3'
//       })
//     });

//     if (!response.ok) {
//       throw new Error('Failed to generate speech');
//     }

//     const audioBlob = await response.blob();
//     return URL.createObjectURL(audioBlob);
//   } catch (error) {
//     console.error('Speech generation error:', error);
//     throw error;
//   }
// };
import { Sequence, AbsoluteFill, Audio, Img, useCurrentFrame, interpolate } from 'remotion';
import { createTikTokStyleCaptions } from '@remotion/captions';

export const FRAME_RATE = 30;
export const SCENE_DURATION = 5 * FRAME_RATE; // 4 seconds * 30fps = 120 frames

const TRANSITION_DURATION = 30; // Duration of transition in frames

export const MainVideo = ({ scenes, images, audioUrls }) => {
  return (
    <AbsoluteFill>
      {scenes.map((scene, index) => (
        <Sequence
          key={index}
          from={index * SCENE_DURATION - (index > 0 ? TRANSITION_DURATION : 0)}
          durationInFrames={SCENE_DURATION + TRANSITION_DURATION}
        >
          <SceneComponent
            image={images[index]}
            audioData={audioUrls[index]}
            text={scene.contentText}
            words={audioUrls[index]?.words || []}
            isTransitioning={index > 0}
          />
        </Sequence>
      ))}
    </AbsoluteFill>
  );
};

const SceneComponent = ({ image, audioData, text, words, isTransitioning }) => {
  const frame = useCurrentFrame();
  const currentTimeInSeconds = frame / FRAME_RATE;

  // Create TikTok-style captions
  const captions = createTikTokStyleCaptions({
    words: words.map((word) => ({
      start: word.start,
      end: word.end,
      text: word.text,
    })),
    currentTime: currentTimeInSeconds,
    style: {
      fontSize: 45,
      fontFamily: 'Arial',
      fontWeight: 'bold',
      color: 'white',
      textShadow: '2px 2px 4px rgba(0,0,0,0.8)',
    },
    containerStyle: {
      position: 'absolute',
      bottom: '20%',
      width: '100%',
      textAlign: 'center',
      display: 'flex',
      flexDirection: 'column',
      alignItems: 'center',
      padding: '0 20px',
    },
    highlightStyle: {
      backgroundColor: 'rgba(255, 255, 255, 0.2)',
      borderRadius: '4px',
      padding: '4px 8px',
    }
  });

  return (
    <AbsoluteFill>
      <Img
        src={image}
        style={{
          width: '100%',
          height: '100%',
          objectFit: 'cover',
          objectPosition: 'center',
        }}
      />
      {audioData?.audioUrl && <Audio src={audioData.audioUrl} />}
      
      {/* Render the TikTok-style captions */}
      <div style={{ position: 'absolute', width: '100%', height: '100%', zIndex: 10 }}>
        {captions}
      </div>

      {/* Debug overlay */}
      <div
        style={{
          position: 'absolute',
          bottom: 20,
          left: 20,
          color: 'white',
          fontSize: '16px',
          backgroundColor: 'rgba(0,0,0,0.5)',
          padding: '10px',
          zIndex: 11,
        }}
      >
        Time: {currentTimeInSeconds.toFixed(2)}s
        <br />
        Duration: {audioData?.duration}s
      </div>
    </AbsoluteFill>
  );
}; 

// export const generateSpeech = async (contentText) => {
//   try {
//     const response = await fetch('https://api.openai.com/v1/audio/speech', {
//       method: 'POST',
//       headers: {
//         'Content-Type': 'application/json',
//         'Authorization': process.env.AUDIO_API_KEY
//       },
//       body: JSON.stringify({
//         model: 'tts-1',
//         voice: 'alloy',
//         input: contentText,
//         speed: 1.15, // Consistent speed
//         response_format: 'mp3'
//       })
//     });

//     if (!response.ok) {
//       throw new Error('Failed to generate speech');
//     }

//     const audioBlob = await response.blob();
//     return URL.createObjectURL(audioBlob);
//   } catch (error) {
//     console.error('Speech generation error:', error);
//     throw error;
//   }
// };

// export const generateAudio = async (contentText) => {
//   try {
//     // First generate the audio
//     const audioResponse = await fetch('https://api.openai.com/v1/audio/speech', {
//       method: 'POST',
//       headers: {
//         'Content-Type': 'application/json',
//         'Authorization': process.env.OPENAI_API_KEY,
//       }, 
//       body: JSON.stringify({
//         model: 'tts-1',
//         voice: 'alloy',
//         input: contentText,
//       }),
//     });

//     if (!audioResponse.ok) {
//       throw new Error(`Audio generation failed: ${audioResponse.statusText}`);
//     }

//     const audioBlob = await audioResponse.blob();
//     const audioUrl = URL.createObjectURL(audioBlob);

//     // Create form data for the STT request
//     const formData = new FormData();
//     formData.append('file', audioBlob, 'audio.mp3');
//     formData.append('model', 'whisper-1');
//     formData.append('response_format', 'json');
//     formData.append('timestamps', 'word');

//     // Get word-level timestamps using Whisper API
//     const sttResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
//       method: 'POST',
//       headers: {
    //         'Authorization':
    //       },
//       body: formData,
//     });

//     if (!sttResponse.ok) {
//       const errorText = await sttResponse.text();
//       console.error('STT API Error:', errorText);
//       throw new Error(`STT processing failed: ${sttResponse.statusText}`);
//     }

//     const transcription = await sttResponse.json();
//     console.log('Full Transcription Response:', transcription);

//     if (!transcription.words) {
//       throw new Error('Word-level timestamps not provided by the API');
//     }

//     const words = transcription.words.map(word => ({
//       word: word.word,
//       start: word.start,
//       end: word.end
//     }));

//     return {
//       audioUrl,
//       totalDuration: words[words.length - 1]?.end || 0,
//       words
//     };
//   } catch (error) {
//     console.error('Error generating audio with timestamps:', error);
//     throw error;
//   }
// }; //ok 

